{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53940, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of        carat        cut color clarity  depth  table  price     x     y     z\n",
       "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
       "53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n",
       "53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n",
       "53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n",
       "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
       "\n",
       "[53940 rows x 10 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940</td>\n",
       "      <td>53940</td>\n",
       "      <td>53940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>21551</td>\n",
       "      <td>11292</td>\n",
       "      <td>13065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cut  color clarity\n",
       "count   53940  53940   53940\n",
       "unique      5      7       8\n",
       "top     Ideal      G     SI1\n",
       "freq    21551  11292   13065"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.describe(exclude=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract feature and target arrays\n",
    "X, y = diamonds.drop('price', axis=1), diamonds[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Convert to Pandas category\n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carat       float64\n",
       "cut        category\n",
       "color      category\n",
       "clarity    category\n",
       "depth       float64\n",
       "table       float64\n",
       "x           float64\n",
       "y           float64\n",
       "z           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen objective function and any other hyperparameters of XGBoost should be specified in a dictionary, which by convention should be called params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this initial params, we are also setting tree_method to gpu_hist, which enables GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set another parameter called num_boost_round, which stands for number of boosting rounds. Internally, XGBoost minimizes the loss function RMSE in small incremental rounds (more on this later). This parameter specifies the amount of those rounds. The ideal number of rounds is found through hyperparameter tuning. For now, we will just set it to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the boosting rounds, the model object has learned all the patterns of the training set it possibly can. Now, we must measure its performance by testing it on unseen data. That's where our dtest_reg DMatrix comes into play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = model.predict(dtest_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step of the process is called model evaluation (or inference). Once you generate predictions with predict, you pass them inside mean_squared_error function of Sklearn to compare against y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the base model: 555.607\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve got a base score ~543$, which was the performance of a base model with default parameters. There are two ways we can improve it— by performing cross-validation and hyperparameter tuning. But before that, let’s see a quicker way of evaluating XGBoost models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Validation Sets During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a machine learning model is like launching a rocket into space. You can control everything about the model up to the launch, but once it does, all you can do is stand by and wait for it to finish.\n",
    "\n",
    "But the problem with our current training process is that we can’t even watch where the model is going. To solve this, we will use evaluation arrays that allow us to see model performance as it gets improved incrementally across boosting rounds.\n",
    "\n",
    "First, let’s set up the parameters again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a list of two tuples that each contain two elements. The first element is the array for the model to evaluate, and the second is the array’s name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass this array to the evals parameter of xgb.train, we will see the model performance after each boosting round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2874.29379\tvalidation-rmse:2817.38773\n",
      "[1]\ttrain-rmse:2092.07711\tvalidation-rmse:2054.73630\n",
      "[2]\ttrain-rmse:1549.52687\tvalidation-rmse:1526.30592\n",
      "[3]\ttrain-rmse:1184.46798\tvalidation-rmse:1174.90119\n",
      "[4]\ttrain-rmse:941.09127\tvalidation-rmse:943.28272\n",
      "[5]\ttrain-rmse:784.58014\tvalidation-rmse:796.09651\n",
      "[6]\ttrain-rmse:685.75110\tvalidation-rmse:705.22245\n",
      "[7]\ttrain-rmse:624.67281\tvalidation-rmse:653.32563\n",
      "[8]\ttrain-rmse:584.19599\tvalidation-rmse:620.30404\n",
      "[9]\ttrain-rmse:558.77667\tvalidation-rmse:599.24504\n",
      "[10]\ttrain-rmse:543.85303\tvalidation-rmse:586.99790\n",
      "[11]\ttrain-rmse:531.92694\tvalidation-rmse:578.68120\n",
      "[12]\ttrain-rmse:523.08456\tvalidation-rmse:571.73527\n",
      "[13]\ttrain-rmse:515.67753\tvalidation-rmse:567.19913\n",
      "[14]\ttrain-rmse:510.77594\tvalidation-rmse:564.66402\n",
      "[15]\ttrain-rmse:506.68519\tvalidation-rmse:563.21547\n",
      "[16]\ttrain-rmse:502.96796\tvalidation-rmse:561.80880\n",
      "[17]\ttrain-rmse:498.90184\tvalidation-rmse:560.36561\n",
      "[18]\ttrain-rmse:492.74859\tvalidation-rmse:558.46274\n",
      "[19]\ttrain-rmse:490.30278\tvalidation-rmse:556.87216\n",
      "[20]\ttrain-rmse:487.42071\tvalidation-rmse:556.44229\n",
      "[21]\ttrain-rmse:484.74496\tvalidation-rmse:556.55429\n",
      "[22]\ttrain-rmse:480.95735\tvalidation-rmse:557.84139\n",
      "[23]\ttrain-rmse:478.48520\tvalidation-rmse:557.89540\n",
      "[24]\ttrain-rmse:475.23956\tvalidation-rmse:557.37962\n",
      "[25]\ttrain-rmse:471.61791\tvalidation-rmse:556.87508\n",
      "[26]\ttrain-rmse:469.65231\tvalidation-rmse:556.70128\n",
      "[27]\ttrain-rmse:466.45165\tvalidation-rmse:555.73740\n",
      "[28]\ttrain-rmse:464.66200\tvalidation-rmse:555.11206\n",
      "[29]\ttrain-rmse:463.36324\tvalidation-rmse:555.09142\n",
      "[30]\ttrain-rmse:460.86396\tvalidation-rmse:554.68339\n",
      "[31]\ttrain-rmse:459.68274\tvalidation-rmse:554.79977\n",
      "[32]\ttrain-rmse:457.48581\tvalidation-rmse:554.57599\n",
      "[33]\ttrain-rmse:455.07939\tvalidation-rmse:555.65575\n",
      "[34]\ttrain-rmse:454.03028\tvalidation-rmse:555.26394\n",
      "[35]\ttrain-rmse:452.35989\tvalidation-rmse:554.62246\n",
      "[36]\ttrain-rmse:449.38579\tvalidation-rmse:553.35840\n",
      "[37]\ttrain-rmse:448.64842\tvalidation-rmse:553.45248\n",
      "[38]\ttrain-rmse:448.47633\tvalidation-rmse:553.35233\n",
      "[39]\ttrain-rmse:445.58979\tvalidation-rmse:553.05158\n",
      "[40]\ttrain-rmse:444.03762\tvalidation-rmse:552.62130\n",
      "[41]\ttrain-rmse:442.01167\tvalidation-rmse:552.97558\n",
      "[42]\ttrain-rmse:441.35357\tvalidation-rmse:553.23244\n",
      "[43]\ttrain-rmse:440.74163\tvalidation-rmse:553.14153\n",
      "[44]\ttrain-rmse:440.60815\tvalidation-rmse:553.05782\n",
      "[45]\ttrain-rmse:439.48758\tvalidation-rmse:553.30981\n",
      "[46]\ttrain-rmse:438.70697\tvalidation-rmse:553.22313\n",
      "[47]\ttrain-rmse:435.38239\tvalidation-rmse:553.74845\n",
      "[48]\ttrain-rmse:434.17988\tvalidation-rmse:553.24786\n",
      "[49]\ttrain-rmse:432.53983\tvalidation-rmse:553.43480\n",
      "[50]\ttrain-rmse:430.07110\tvalidation-rmse:553.50718\n",
      "[51]\ttrain-rmse:429.02843\tvalidation-rmse:553.68181\n",
      "[52]\ttrain-rmse:428.82789\tvalidation-rmse:553.55179\n",
      "[53]\ttrain-rmse:426.65097\tvalidation-rmse:554.33720\n",
      "[54]\ttrain-rmse:425.35817\tvalidation-rmse:555.00412\n",
      "[55]\ttrain-rmse:424.43950\tvalidation-rmse:555.04530\n",
      "[56]\ttrain-rmse:423.98886\tvalidation-rmse:555.23862\n",
      "[57]\ttrain-rmse:423.37385\tvalidation-rmse:555.01524\n",
      "[58]\ttrain-rmse:422.89152\tvalidation-rmse:554.73559\n",
      "[59]\ttrain-rmse:420.17877\tvalidation-rmse:555.28689\n",
      "[60]\ttrain-rmse:418.57995\tvalidation-rmse:555.44368\n",
      "[61]\ttrain-rmse:416.15098\tvalidation-rmse:556.22395\n",
      "[62]\ttrain-rmse:415.20122\tvalidation-rmse:556.03493\n",
      "[63]\ttrain-rmse:413.94128\tvalidation-rmse:555.89869\n",
      "[64]\ttrain-rmse:412.14760\tvalidation-rmse:555.56320\n",
      "[65]\ttrain-rmse:412.07284\tvalidation-rmse:555.51809\n",
      "[66]\ttrain-rmse:411.12288\tvalidation-rmse:556.53856\n",
      "[67]\ttrain-rmse:409.86233\tvalidation-rmse:556.02134\n",
      "[68]\ttrain-rmse:408.98845\tvalidation-rmse:555.64995\n",
      "[69]\ttrain-rmse:407.60369\tvalidation-rmse:555.40507\n",
      "[70]\ttrain-rmse:406.77489\tvalidation-rmse:555.06703\n",
      "[71]\ttrain-rmse:405.55576\tvalidation-rmse:554.85987\n",
      "[72]\ttrain-rmse:404.01644\tvalidation-rmse:554.78435\n",
      "[73]\ttrain-rmse:402.82464\tvalidation-rmse:554.70160\n",
      "[74]\ttrain-rmse:401.31075\tvalidation-rmse:554.40804\n",
      "[75]\ttrain-rmse:399.88578\tvalidation-rmse:554.56453\n",
      "[76]\ttrain-rmse:399.46555\tvalidation-rmse:554.83544\n",
      "[77]\ttrain-rmse:398.97065\tvalidation-rmse:554.83387\n",
      "[78]\ttrain-rmse:398.50360\tvalidation-rmse:554.66181\n",
      "[79]\ttrain-rmse:396.24144\tvalidation-rmse:555.31061\n",
      "[80]\ttrain-rmse:394.18070\tvalidation-rmse:555.00800\n",
      "[81]\ttrain-rmse:392.71336\tvalidation-rmse:555.25449\n",
      "[82]\ttrain-rmse:392.14361\tvalidation-rmse:555.18259\n",
      "[83]\ttrain-rmse:390.73064\tvalidation-rmse:555.24386\n",
      "[84]\ttrain-rmse:390.32572\tvalidation-rmse:555.02768\n",
      "[85]\ttrain-rmse:388.65251\tvalidation-rmse:555.33841\n",
      "[86]\ttrain-rmse:387.44200\tvalidation-rmse:555.24404\n",
      "[87]\ttrain-rmse:386.96057\tvalidation-rmse:555.09808\n",
      "[88]\ttrain-rmse:385.78554\tvalidation-rmse:554.95239\n",
      "[89]\ttrain-rmse:384.05170\tvalidation-rmse:555.95636\n",
      "[90]\ttrain-rmse:382.65353\tvalidation-rmse:555.74725\n",
      "[91]\ttrain-rmse:381.97534\tvalidation-rmse:555.69323\n",
      "[92]\ttrain-rmse:380.47646\tvalidation-rmse:555.26134\n",
      "[93]\ttrain-rmse:379.11827\tvalidation-rmse:554.93934\n",
      "[94]\ttrain-rmse:377.82598\tvalidation-rmse:555.07593\n",
      "[95]\ttrain-rmse:376.66849\tvalidation-rmse:555.53397\n",
      "[96]\ttrain-rmse:374.97521\tvalidation-rmse:555.60914\n",
      "[97]\ttrain-rmse:374.61227\tvalidation-rmse:555.61898\n",
      "[98]\ttrain-rmse:373.84881\tvalidation-rmse:555.65889\n",
      "[99]\ttrain-rmse:373.74308\tvalidation-rmse:555.60692\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world projects, you usually train for thousands of boosting rounds, which means that many rows of output. To reduce them, you can use the verbose_eval parameter, which forces XGBoost to print performance updates every vebose_eval rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:2817.38773\ttrain-rmse:2874.29379\n",
      "[10]\tvalidation-rmse:586.99790\ttrain-rmse:543.85303\n",
      "[20]\tvalidation-rmse:556.44229\ttrain-rmse:487.42071\n",
      "[30]\tvalidation-rmse:554.68339\ttrain-rmse:460.86396\n",
      "[40]\tvalidation-rmse:552.62130\ttrain-rmse:444.03762\n",
      "[50]\tvalidation-rmse:553.50718\ttrain-rmse:430.07110\n",
      "[60]\tvalidation-rmse:555.44368\ttrain-rmse:418.57995\n",
      "[70]\tvalidation-rmse:555.06703\ttrain-rmse:406.77489\n",
      "[80]\tvalidation-rmse:555.00800\ttrain-rmse:394.18070\n",
      "[90]\tvalidation-rmse:555.74725\ttrain-rmse:382.65353\n",
      "[99]\tvalidation-rmse:555.60692\ttrain-rmse:373.74308\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "n = 100\n",
    "\n",
    "evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n",
    "\n",
    "\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval=10 # Every ten rounds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you must have realized how important boosting rounds are. Generally, the more rounds there are, the more XGBoost tries to minimize the loss. But this doesn’t mean the loss will always go down. Let’s try with 5000 boosting rounds with the verbosity of 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:2817.38773\ttrain-rmse:2874.29379\n",
      "[250]\tvalidation-rmse:561.30944\ttrain-rmse:276.85755\n",
      "[500]\tvalidation-rmse:563.29248\ttrain-rmse:197.72375\n",
      "[750]\tvalidation-rmse:568.62872\ttrain-rmse:150.03843\n",
      "[1000]\tvalidation-rmse:572.18689\ttrain-rmse:121.69016\n",
      "[1250]\tvalidation-rmse:574.18885\ttrain-rmse:99.34243\n",
      "[1500]\tvalidation-rmse:576.42698\ttrain-rmse:84.17960\n",
      "[1750]\tvalidation-rmse:578.10716\ttrain-rmse:71.14960\n",
      "[2000]\tvalidation-rmse:578.58142\ttrain-rmse:60.88001\n",
      "[2250]\tvalidation-rmse:579.16086\ttrain-rmse:52.80762\n",
      "[2500]\tvalidation-rmse:579.96944\ttrain-rmse:46.47975\n",
      "[2750]\tvalidation-rmse:580.51823\ttrain-rmse:41.25980\n",
      "[3000]\tvalidation-rmse:580.78514\ttrain-rmse:36.59124\n",
      "[3250]\tvalidation-rmse:581.21700\ttrain-rmse:32.91052\n",
      "[3500]\tvalidation-rmse:581.36774\ttrain-rmse:29.47363\n",
      "[3750]\tvalidation-rmse:581.61295\ttrain-rmse:26.40702\n",
      "[4000]\tvalidation-rmse:581.86083\ttrain-rmse:23.90580\n",
      "[4250]\tvalidation-rmse:581.89800\ttrain-rmse:21.68452\n",
      "[4500]\tvalidation-rmse:582.07031\ttrain-rmse:19.97500\n",
      "[4750]\tvalidation-rmse:582.09360\ttrain-rmse:18.48141\n",
      "[4999]\tvalidation-rmse:582.23127\ttrain-rmse:17.15835\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "n = 5000\n",
    "\n",
    "evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n",
    "\n",
    "\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the lowest loss before round 500. After that, even though training loss keeps going down, the validation loss (the one we care about) keeps increasing.\n",
    "\n",
    "When given an unnecessary number of boosting rounds, XGBoost starts to overfit and memorize the dataset. This, in turn, leads to validation performance drop because the model is memorizing instead of generalizing.\n",
    "\n",
    "Remember, we want the golden middle: a model that learned just enough patterns in training that it gives the highest performance on the validation set. So, how do we find the perfect number of boosting rounds, then?\n",
    "\n",
    "We will use a technique called early stopping. Early stopping forces XGBoost to watch the validation loss, and if it stops improving for a specified number of rounds, it automatically stops training.\n",
    "\n",
    "This means we can set as high a number of boosting rounds as long as we set a sensible number of early stopping rounds.\n",
    "\n",
    "For example, let’s use 10000 boosting rounds and set the early_stopping_rounds parameter to 50. This way, XGBoost will automatically stop the training if validation loss doesn't improve for 50 consecutive rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:2817.38773\ttrain-rmse:2874.29379\n",
      "[250]\tvalidation-rmse:561.30944\ttrain-rmse:276.85755\n",
      "[500]\tvalidation-rmse:563.29248\ttrain-rmse:197.72375\n",
      "[750]\tvalidation-rmse:568.62872\ttrain-rmse:150.03843\n",
      "[1000]\tvalidation-rmse:572.18689\ttrain-rmse:121.69016\n",
      "[1250]\tvalidation-rmse:574.18885\ttrain-rmse:99.34243\n",
      "[1500]\tvalidation-rmse:576.42698\ttrain-rmse:84.17960\n",
      "[1750]\tvalidation-rmse:578.10716\ttrain-rmse:71.14960\n",
      "[2000]\tvalidation-rmse:578.58142\ttrain-rmse:60.88001\n",
      "[2250]\tvalidation-rmse:579.16086\ttrain-rmse:52.80762\n",
      "[2500]\tvalidation-rmse:579.96944\ttrain-rmse:46.47975\n",
      "[2750]\tvalidation-rmse:580.51823\ttrain-rmse:41.25980\n",
      "[3000]\tvalidation-rmse:580.78514\ttrain-rmse:36.59124\n",
      "[3250]\tvalidation-rmse:581.21700\ttrain-rmse:32.91052\n",
      "[3500]\tvalidation-rmse:581.36774\ttrain-rmse:29.47363\n",
      "[3750]\tvalidation-rmse:581.61295\ttrain-rmse:26.40702\n",
      "[4000]\tvalidation-rmse:581.86083\ttrain-rmse:23.90580\n",
      "[4250]\tvalidation-rmse:581.89800\ttrain-rmse:21.68452\n",
      "[4500]\tvalidation-rmse:582.07031\ttrain-rmse:19.97500\n",
      "[4750]\tvalidation-rmse:582.09360\ttrain-rmse:18.48141\n",
      "[5000]\tvalidation-rmse:582.23123\ttrain-rmse:17.15711\n",
      "[5250]\tvalidation-rmse:582.29156\ttrain-rmse:15.99624\n",
      "[5500]\tvalidation-rmse:582.40910\ttrain-rmse:15.00659\n",
      "[5750]\tvalidation-rmse:582.44321\ttrain-rmse:14.15176\n",
      "[6000]\tvalidation-rmse:582.49056\ttrain-rmse:13.42440\n",
      "[6250]\tvalidation-rmse:582.55184\ttrain-rmse:12.83556\n",
      "[6500]\tvalidation-rmse:582.54236\ttrain-rmse:12.33491\n",
      "[6750]\tvalidation-rmse:582.55523\ttrain-rmse:11.91651\n",
      "[7000]\tvalidation-rmse:582.57662\ttrain-rmse:11.51327\n",
      "[7250]\tvalidation-rmse:582.62320\ttrain-rmse:11.19055\n",
      "[7500]\tvalidation-rmse:582.65106\ttrain-rmse:10.90900\n",
      "[7750]\tvalidation-rmse:582.66627\ttrain-rmse:10.67841\n",
      "[8000]\tvalidation-rmse:582.67820\ttrain-rmse:10.47660\n",
      "[8250]\tvalidation-rmse:582.70346\ttrain-rmse:10.30108\n",
      "[8500]\tvalidation-rmse:582.72539\ttrain-rmse:10.16553\n",
      "[8750]\tvalidation-rmse:582.73385\ttrain-rmse:10.03847\n",
      "[9000]\tvalidation-rmse:582.75673\ttrain-rmse:9.94908\n",
      "[9250]\tvalidation-rmse:582.78424\ttrain-rmse:9.86601\n",
      "[9500]\tvalidation-rmse:582.79386\ttrain-rmse:9.79081\n",
      "[9750]\tvalidation-rmse:582.80453\ttrain-rmse:9.72182\n",
      "[9999]\tvalidation-rmse:582.81551\ttrain-rmse:9.67083\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "n = 10000\n",
    "\n",
    "evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n",
    "\n",
    "\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval=250,\n",
    "   # activate early stopping\n",
    "   early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cross-validation, we still have two sets: training and testing. While the test set waits in the corner, we split the training into 3, 5, 7, or k splits or folds. Then, we train the model k times. Each time, we use k-1 parts for training and the final kth part for validation. This process is called k-fold cross-validation. After all folds are done, we can take the mean of the scores as the final, most realistic performance of the model. Let’s perform this process in code using the cv function of XGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "n = 1000\n",
    "\n",
    "results = xgb.cv(\n",
    "   params, dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   early_stopping_rounds=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2874.530912</td>\n",
       "      <td>9.576510</td>\n",
       "      <td>2877.437274</td>\n",
       "      <td>37.093540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2089.327469</td>\n",
       "      <td>8.317290</td>\n",
       "      <td>2094.021636</td>\n",
       "      <td>24.828795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550.617973</td>\n",
       "      <td>5.223297</td>\n",
       "      <td>1558.386252</td>\n",
       "      <td>18.540267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1183.812759</td>\n",
       "      <td>5.193420</td>\n",
       "      <td>1195.032441</td>\n",
       "      <td>13.471580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>941.203113</td>\n",
       "      <td>4.539805</td>\n",
       "      <td>958.728828</td>\n",
       "      <td>9.479449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0      2874.530912        9.576510     2877.437274      37.093540\n",
       "1      2089.327469        8.317290     2094.021636      24.828795\n",
       "2      1550.617973        5.223297     1558.386252      18.540267\n",
       "3      1183.812759        5.193420     1195.032441      13.471580\n",
       "4       941.203113        4.539805      958.728828       9.479449"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the same number of rows as the number of boosting rounds. Each row is the average of all splits for that round. So, to find the best score, we take the minimum of the test-rmse-mean column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse = results['test-rmse-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method of cross-validation is used to see the true performance of the model. Once satisfied with its score, you must retrain it on the full data before deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an XGBoost classifier is as easy as changing the objective function; the rest can stay the same.\n",
    "\n",
    "The two most popular classification objectives are:\n",
    "\n",
    "binary:logistic - binary classification (the target contains only two classes, i.e., cat or dog)\n",
    "multi:softprob - multi-class classification (more than two classes in the target, i.e., apple/orange/banana)\n",
    "Performing binary and multi-class classification in XGBoost is almost identical, so we will go with the latter. Let’s prepare the data for the task first.\n",
    "\n",
    "We want to predict the cut quality of diamonds given their price and their physical measurements. So, we will build the feature/target arrays accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]\n",
    "\n",
    "# Encode y to numeric\n",
    "y_encoded = OrdinalEncoder().fit_transform(y)\n",
    "\n",
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Convert to pd.Categorical\n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category')\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that since XGBoost only accepts numbers in the target, we are encoding the text classes in the target with OrdinalEncoder of Sklearn. Now, we build the DMatrices…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification matrices\n",
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…and set the objective to multi:softprob. This objective also requires the number of classes to be set by us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multi:softprob\", \"tree_method\": \"gpu_hist\", \"num_class\": 5}\n",
    "n = 1000\n",
    "\n",
    "results = xgb.cv(\n",
    "   params, dtrain_clf,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   metrics=[\"mlogloss\", \"auc\", \"merror\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During cross-validation, we are asking XGBoost to watch three classification metrics which report model performance from three different angles. Here is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train-mlogloss-mean', 'train-mlogloss-std', 'train-auc-mean',\n",
       "       'train-auc-std', 'train-merror-mean', 'train-merror-std',\n",
       "       'test-mlogloss-mean', 'test-mlogloss-std', 'test-auc-mean',\n",
       "       'test-auc-std', 'test-merror-mean', 'test-merror-std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the best AUC score, we take the maximum of test-auc-mean column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402233623451636"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test-auc-mean'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Native vs. XGBoost Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been using the native XGBoost API, but its Sklearn API is pretty popular as well.\n",
    "\n",
    "Sklearn is a vast framework with many machine learning algorithms and utilities and has an API syntax loved by almost everyone. Therefore, XGBoost also offers XGBClassifier and XGBRegressor classes so that they can be integrated into the Sklearn ecosystem (at the loss of some of the functionality).\n",
    "\n",
    "If you want to only use the Scikit-learn API whenever possible and only switch to native when you need access to extra functionality, there is a way.\n",
    "\n",
    "After training the XGBoost classifier or regressor, you can convert it using the get_booster method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Train a model using the scikit-learn API\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3, enable_categorical=True)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Convert the model to a native API model\n",
    "model = xgb_classifier.get_booster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
